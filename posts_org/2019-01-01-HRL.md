---
layout: inner
title: 'Hierarchical Reinforcement Learning for Robustness, Performance and Explainability'
date: 2019-01-01 14:15:00
categories: development
type: project
tags: Hierarchical-RL Python DQN
featured_image: 'https://github.com/NotAnyMike/HRL/blob/master/img/gif.gif?raw=true'
project_link: 'https://github.com/notanymike/HRL'
button_icon: 'github'
button_text: 'Visit Project'
lead_text: 'Using Hierarchical Policies in order to improve the general performance of RL models'



---

# Hierarchical Reinforcement Learning for Robustness, Performance and Explainability

This is my MSc Thesis / University of Edinburgh[^1][^2]. Here is a short video:

<iframe width="560" height="315" src="https://www.youtube.com/embed/FVJkvy9j-2g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
In general, a Hierarchical Reinforcement Learning is a group of RL models organised in a hierarchy. Every model is specialised in certain actions, and there is one model which takes care of solving the main task, usually called the “controller”. Given the hierarchy organisation, every model in a higher level uses some of the policies (or models) in lower levels to accomplish its specialised task. More specifically there are many different types of HRL models, some specialised in learning the policies, some others in learning the hierarchy, etc (to see a more in-depth description of HRL see this interesting article from The Gradient, [link](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning){:target="_blank"}). Each one uses different concepts to solve the problem. In this case, I am focusing on pre-trained pre-specified hierarchy to test different characteristics of such models. 

![Hierarchy](/img/posts/HRL/Hierarchy.png)*<center>Figure 1: Hierarchy</center>*

Take for example the hierarchy (figure 1) could be used to drive a car in a simplified environment, here the policy “Avoid Obstacle” can make use of the sub-policies “Change Lane” or “Stay in Lane” to accomplish its goal. In the end, every policy uses a sub-policy or directly the raw actions of the agent, in this case, accelerate, brake, turn left, right or do nothing.

![Example](https://github.com/NotAnyMike/HRL/blob/master/img/gif.gif?raw=true)*<center>Figure 2: HIerarchical Agent and its activations in the hierarchy</center>*

There are several benefits to use HRL. We explore the benefits regarding Robustness, Explanibility and Performance. Thanks to the modularity coming from using a Hierarchy of Policies, for example, if the environment changes, the whole hierarchy of policies do not have to change as well, just the affected part, which can mean significant improvements in training times depending on the complexity. Another example is to improve its performance, suppose we found a policy which is not behaving as expected in a specific case, the solution is to modify somehow (e.g. more training) only that policy, and all the other policies stay the same in the hierarchy.

You can run the model above with little steps in the repository of the project. I invite you to visit it: [https://github.com/NotAnyMike/HRL](https://github.com/NotAnyMike/HRL){:target="_blank"}

------

[^1]: Currently working on this project, I will keep updating this post based on the progress of the thesis.
[^2]: The cover picture is taken from [the repo of the project](<https://arxiv.org/pdf/1710.09767.pdf>){:target="_blank"}